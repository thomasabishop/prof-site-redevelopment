---
date: 2021-05-15
title: 'Algorithmic time complexity'
template: post
featured_image: '../img/post/featured/algorithms.png'
intro: ''
tag_color: 'mediumseagreen'
category:
  - Learning out loud
tags:
  - Computer Science
---

import AlgorithmDistributionChart from '../graphs/algorithmDistribution.jsx';
import { linearTime } from '../graphs/distributions.js';

## Distinguishing algorithms from programs

Algorithms are general sets of instructions that take data in one state, follow a prescribed series of steps and return data in another state. Programs are a specific application of one or more algorithms to achieve an outcome in a specific context. When we think about the relative complexity of algorithms, the actual detail of the steps is mostly abstracted and it is irrelevant to what end the algorithm is being put. For instance you may create a program that returns addresses from a database using a postcode. It is irrelevant to the efficiency of the algorithm whether or not you are looking up postcodes or some other form of alphanumeric string.

## Algorithmic efficiency

Algorithms can be classified based on their efficiency. In the context of time complexity, efficiency is function of the algorithm's speed at runtime. However, we should not assume this means that "the fastest algorithms are best". We must be attentive to the particular use case. If we are landing the Curiosity Rover on Mars, we may choose an algorithm that is slower on average in return for a guarentee that it returns a value to some _n_ th degree of accuracy. In other cases for example a video game, we may choose an algorithm that keeps the average time down, even if this occasionally leads to processes that need to be aborted because they take too long.

We need a generalised measure of efficiency to compare algorithms. We can't simply use the number of steps, since some steps will be quicker to complete than others in the course of the overall algorithm and may take longer on different machines with variant hardware. Moreover, in practice the same algorithm will run at marginally different speeds on the _same_ machine, depending on its internal state at the given time that it run. So we use the following criterion: **the number of steps required relative to the input**.

> Two given computers may differ in how quickly they can run an algorithm depending on clock speed, available memory and so forth. They will however tend to require approximately the same number of instructions and we can measure the rate at which the number of instructions increases with the problem size.

This is what the term [asymptotic runtime](https://en.wikipedia.org/wiki/Asymptotic_computational_complexity) means: the rate at which the runtime of an algorithm grows compared to the size of its input. For precision and accuracy we use the worst case scenario as the benchmark.

So, more formally: the efficiency of algorithm _A_ can be contrasted with the efficiency of algorithm _B_ based on the rate at which the runtime of _A_ grows relative to its input, compared to the same property in _B_ (assuming the worst possible performance).

We will find that for the runtime of some algorithms, the size of the input does not change the execution time. In these cases, the runtime is proportional to the input quantity. For other cases, this will not hold true: we will see that there is a relationship between input size and execution time such that the length of the input affects the amount of work that needs to be performed on each item at execution.

### Note

From now on we will use the word 'input' to denote the data that the algorithm 'receives' (in most cases we will conceive this as an array containing a certain data type) and 'execution' to denote the computation that is applied by the algorithm to each item of the data input. Rephrasing the above with these terms we can say that **'algorithmic efficiency' is a measure that describes the rate at which the execution time of an algorithm increases relative to the size of its input**.

## Linear time complexity

<AlgorithmDistributionChart props={linearTime} />
Before we can introduce the famous 'Big O' notation to represent algorithmic complexity,
we first need a concrete example of a specific complexity which we will then express
using formal notation. We'll start with linear time as it is the most intuitive to
grasp.

Let's take a simple function that takes a sequence of integers and returns their sum:

```js
function findSum(array){
	let total = 0;
	for (let i = 0; i < arr.length; i++) {
		total = total += arr[i];
	)
	return total;
  }
```

The input of this function is an array of integers. It returns the of the integers as the output. Let's say that it takes 1ms for the function to sum an array of two integers. If we passed in an array of four integers, how would this change the runtime? Intuitively the answer is that it would take twice as long. As the time it takes to execute `findSum` doesn't change (it's always the same give or take fractional variances, regardless of whether it's summing `2, 5` or `8, 1000`), we can say that the runtime is as long as the number of integers we pass in. A more general way to say this is that the runtime is equal to size of the input. Thus, for algorithms of the class of which `findSum` is a member: **the runtime is (roughly) proportional to the number of items to be processed**.

We've used an entirely arbitrary value of the time it takes our function to sum two integers: 1ms. This is just so we have a quantity to work from, the actual figure is immaterial to the logic underpinning this time complexity. Anyway, it allows us to plot the following data set:

| Length of input | Runtime (ms) |
| --------------- | ------------ |
| 2               | 1            |
| 4               | 2            |
| 6               | 3            |
| 8               | 4            |
| 10              | 6            |

If we plotted this as a graph it is clear that this is equivalent to a linear distribution:
[INSERT GRAPH]

Algorithms which display this distribution are therefore called **linear algorithms**.

The crucial point is that the amount of time it takes to sum the integers does not increase as the algorithm proceeds and the input size grows. This time remains the same. If it did increase, we would have a fluctuating curve on the graph. This aspect remains constant, only the instructions increase. This is why we have a nice steadily-advancing distribution in the graph.

We can now introduce notation to formalise the algorithmic properties we have been discussing.

## Big O notation

To express how algorithms that run in linear time work formally, we say that:

> it takes some constant amount of time (C) to sum one integer and _n_ times as long to sum _n_ integers

The algebraic expression of this is cn : the constant multiplied by the length of the input. In algorithmic notation, the reference to the constant is always removed. Instead we just use n and combine it with a 'big O' which stands for 'order of complexity'. Likewise, if we have an array of four integers being passed to `findSum` we could technically express it as O(4n), but we don't because we are interested in the general case not the specific details of the runtime. So a linear algorithm is expressed algebraically as $O(n)$ which is read as "oh of n" and means
